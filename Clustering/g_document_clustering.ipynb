{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Document Clustering**"
      ],
      "metadata": {
        "id": "9iCd99hLoMq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset_url = 'https://raw.githubusercontent.com/neeharikasinghsjsu/cmpe255assignments/main/Clustering/dataset/g_document_clustering_movie_reviews.csv'\n",
        "data = pd.read_csv(dataset_url)\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R3jAlpnS2Dh",
        "outputId": "9ab06e42-ddf3-411b-8de3-399c9736b871"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label\n",
            "0  Very silly movie, filled with stupid one liner...      0\n",
            "1  As predictable as a Hallmark card, but not wit...      1\n",
            "2  Only a 9/10 from me, a perfect ten would have ...      1\n",
            "3  After Watergate, Vietnam and the dark days of ...      0\n",
            "4  As long as you keep in mind that the productio...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "MetlZUKqVHkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the label column\n",
        "data = data.drop(columns=['label'])\n",
        "\n",
        "data['text'] = data['text'].str.lower().str.replace('[^a-z0-9\\s]', '', regex=True)\n",
        "\n",
        "# Display the preprocessed data\n",
        "print(data.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r0uJgUHW3EL",
        "outputId": "1ca2bba9-7e06-4d27-b078-28fdf15f9429"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0  very silly movie filled with stupid one liners...\n",
            "1  as predictable as a hallmark card but not with...\n",
            "2  only a 910 from me a perfect ten would have be...\n",
            "3  after watergate vietnam and the dark days of t...\n",
            "4  as long as you keep in mind that the productio...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Embeddings**"
      ],
      "metadata": {
        "id": "lq-w-y1SXhAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "# Function to generate embeddings\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Generate embeddings for each document\n",
        "embeddings = data['text'].apply(get_embedding)\n"
      ],
      "metadata": {
        "id": "fOVqBdy9omzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Document Clustering**"
      ],
      "metadata": {
        "id": "TCMMAHNmpej_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Convert embeddings into a format suitable for clustering\n",
        "embeddings_matrix = np.vstack(embeddings)\n",
        "\n",
        "# Perform clustering\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings_matrix)\n",
        "\n",
        "# Assign the cluster labels to the data\n",
        "data['cluster'] = kmeans.labels_\n",
        "\n",
        "# Display the clustered data\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "7kgugvglpg9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization and Analysis**"
      ],
      "metadata": {
        "id": "Y48P2B4BpjDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce the dimensionality for visualization\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings_matrix)\n",
        "\n",
        "# Plotting the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_clusters):\n",
        "    plt.scatter(reduced_embeddings[data['cluster'] == i, 0], reduced_embeddings[data['cluster'] == i, 1], label=f'Cluster {i}')\n",
        "plt.title('Document Clusters')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dwa4evsFpl18"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}